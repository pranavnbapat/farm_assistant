# =========================
# OpenSearch Configuration
# =========================
OPENSEARCH_API_USR=
OPENSEARCH_API_PWD=
OPENSEARCH_API_URL=
VERIFY_SSL=true

# =========================
# vLLM Configuration (Primary)
# =========================
# vLLM base URL (OpenAI-compatible API)
VLLM_URL=http://localhost:8000
# Alternative: Use RUNPOD_VLLM_HOST if VLLM_URL is not set
RUNPOD_VLLM_HOST=
# Model identifier served by vLLM
VLLM_MODEL=qwen3-30b-a3b-awq
# Max model context length (tokens)
VLLM_MAX_MODEL_LEN=131072
# vLLM API key (OpenAI-compatible, if required)
VLLM_API_KEY=

# =========================
# Legacy Ollama Configuration (Alternative)
# =========================
OLLAMA_URL=http://ollama:11434
LLM_MODEL=deepseek-r1:8b

# =========================
# Common LLM Settings
# =========================
MAX_TOKENS=-1
TEMPERATURE=0.4
NUM_CTX=16384
TOP_K=5
MAX_CONTEXT_CHARS=24000
MAX_OUTPUT_TOKENS=1024
MAX_INPUT_TOKENS=12000
MAX_USER_INPUT_TOKENS=2000

# =========================
# Search Configuration
# =========================
SEARCH_MODEL=msmarco
SEARCH_INCLUDE_FULLTEXT=true
SEARCH_SORT_BY=score_desc
SEARCH_PAGE=1
SEARCH_K=0
SEARCH_DEV=false

# =========================
# Application Settings
# =========================
LOG_LEVEL=INFO
ENABLE_DOCS=true
ENABLE_REDOC=false

GARBAGE_OK_MAX=0.10
CONF_STRONG=0.80

# Environment: local, dev, or prd
FA_ENV=local

# =========================
# Auth Backend (Django)
# =========================
# Optional: Override the default auth backend URL
# If not set, uses the default based on FA_ENV:
#   local: http://127.0.0.1:8000/fastapi/login/
#   dev:   https://backend-admin.dev.farmbook.ugent.be/fastapi/login/
#   prd:   https://backend-admin.prd.farmbook.ugent.be/fastapi/login/
AUTH_BACKEND_URL=

# Admin API token for backend authentication (if required)
ADMIN_API_TOKEN=

# =========================
# Redis Cache
# =========================
REDIS_URL=redis://127.0.0.1:6379/0
CACHE_ENABLED=true
CACHE_TTL_SECONDS=86400
MAX_ACTIVE_GENERATIONS=3

# =========================
# Piper TTS
# =========================
PIPER_MODELS_DIR=/app/models

# =========================
# Pipeline Configuration
# =========================
# Max chars passed downstream (0=unlimited)
PIPELINE_MAX_CHARS=0

# =========================
# Chunking + Request Limits
# =========================
# Target chunk size in tokens
IMPROVER_CHUNK_TARGET_TOK=16000
# Overlap between chunks (tokens)
IMPROVER_CHUNK_OVERLAP_TOK=400
# Default generation cap per call (tokens)
IMPROVER_DEFAULT_NUM_PREDICT=3072
# Cap for "combine" calls (tokens)
IMPROVER_COMBINE_NUM_PREDICT=24576
# Retry attempts per doc (LLM stage)
IMPROVER_MAX_ATTEMPTS=1
# Timeout per LLM request (seconds)
IMPROVER_PER_REQUEST_TIMEOUT=600
# Delay between documents (seconds)
IMPROVER_DOC_DELAY_SEC=0.5
