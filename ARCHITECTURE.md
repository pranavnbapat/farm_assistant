# Architecture Documentation

This document describes the architecture of the Farm Assistant RAG application.

## System Overview

The Farm Assistant is a conversational AI system built on a **Retrieval-Augmented Generation (RAG)** architecture. It combines semantic search over an agricultural knowledge base with large language model inference to provide accurate, source-cited answers.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Client (Browser)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Login Page â”‚â”€â”€â”€â–¶â”‚  Chat UI    â”‚â”€â”€â”€â–¶â”‚  SSE Stream (text + voice)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Farm Assistant (FastAPI)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Auth      â”‚    â”‚   Router    â”‚    â”‚    Streaming Handler        â”‚  â”‚
â”‚  â”‚  (/login)   â”‚    â”‚ (Intent)    â”‚    â”‚      (/ask/stream)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                  â”‚                         â”‚                  â”‚
â”‚         â–¼                  â–¼                         â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                      Processing Pipeline                        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚  Search  â”‚â”€â”€â–¶â”‚ Context  â”‚â”€â”€â–¶â”‚  Prompt  â”‚â”€â”€â–¶â”‚  Generation  â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  (OS)    â”‚   â”‚  Build   â”‚   â”‚  Build   â”‚   â”‚   (vLLM)     â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                           â–¼                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenSearch   â”‚          â”‚     vLLM      â”‚          â”‚    Redis      â”‚
â”‚  (Documents)  â”‚          â”‚    (LLM)      â”‚          â”‚   (Cache)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Django Backend (Auth/Sessions)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Auth/JWT   â”‚   â”‚  Sessions   â”‚   â”‚      User Profile Service      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. FastAPI Application (`app/main.py`)

The main entry point that:
- Initializes the FastAPI application with lifespan management
- Configures CORS middleware
- Mounts static files and templates
- Registers API routers
- Manages concurrency with semaphores
- Provides proxy endpoints for Django backend (avoids CORS issues)

Key features:
- **Lifespan management**: Initializes resources (semaphores) on startup
- **Environment-based routing**: Different backend URLs for local/dev/production
- **JWT authentication**: Proxies authentication to Django backend
- **Session proxying**: Proxies session management calls to Django backend

### 2. Request Flow

#### Chat Request Flow

```
User Query
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Domain Check   â”‚ â”€â”€â”€â”€â–º Validates question is agriculture-related
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Intent Router  â”‚ â”€â”€â”€â”€â–º Routes to RAG or LLM-only
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼ (RAG Path)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenSearch     â”‚ â”€â”€â”€â”€â–º Retrieve relevant documents
â”‚  (Neural Search)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Context Service â”‚ â”€â”€â”€â”€â–º Rank, filter, format documents
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load Profile   â”‚ â”€â”€â”€â”€â–º Get user expertise/farm type/preferences
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Service  â”‚ â”€â”€â”€â”€â–º Build structured prompt with context + profile
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   vLLM LLM      â”‚ â”€â”€â”€â”€â–º Generate streaming response
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Update Profile  â”‚ â”€â”€â”€â”€â–º Extract facts, update user profile
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Response with  â”‚ â”€â”€â”€â”€â–º Citations extracted and linked
â”‚  Source Links   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Streaming Architecture

The application uses **Server-Sent Events (SSE)** for real-time streaming:

1. Client establishes SSE connection to `/ask/stream`
2. Server emits events:
   - `status`: Processing stage updates (Intent, Search, Context, LLM)
   - `token`: Individual response tokens
   - `sources`: Retrieved source documents (filtered by citations)
   - `stats`: LLM generation statistics
   - `timing`: Performance metrics
   - `done`: Completion signal
   - `error`: Error information

### 3. Services Layer

#### Search Service (`app/services/search_service.py`)

Orchestrates document retrieval:
- Builds search payloads for OpenSearch
- Handles pagination
- Collects and deduplicates results
- Provides hit probing for intent classification

#### Context Service (`app/services/context_service.py`)

Transforms search results into LLM context:
- **Paragraph splitting**: Divides documents into semantic chunks
- **Ranking**: Scores paragraphs by query term overlap and keyword boosts
- **Context building**: Formats documents with metadata (title, source, license, project)
- **Source tracking**: Maintains citation indices for answer attribution
- **Retrieval quality estimation**: Simple token overlap scoring

Key algorithm:
```python
# Token overlap scoring
q_tokens = extract_tokens(question)
for paragraph in document:
    p_tokens = extract_tokens(paragraph)
    score = overlap(q_tokens, p_tokens) * 10 + 
            boost_overlap(p_tokens, keywords) * 4 +
            position_bonus
```

#### Prompt Service (`app/services/prompt_service.py`)

Manages prompt templates:
- **RAG Prompt**: Includes sources, citation instructions, domain restriction, language rules
- **Generic Prompt**: For LLM-only queries without retrieval
- **Summary Prompt**: For text summarization tasks
- **Title Prompt**: For automatic session title generation

Key features:
- **Domain restriction**: Strictly limits responses to agriculture topics
- **Language rules**: Instructs model to respond in user's question language
- **Personalization**: Tailors responses based on user profile

#### User Profile Service (`app/services/user_profile_service.py`)

Manages user personalization:
- **Profile extraction**: Keyword-based extraction of user attributes
- **Fact storage**: Stores user facts in Django backend
- **Profile context building**: Creates context string for prompts

Extracted attributes:
- Expertise level (beginner/expert)
- Farm type (organic, conventional, dairy, etc.)
- Region and crops
- Communication preferences
- Topics of interest

Extraction methods:
1. **Keyword-based** (current): Fast, reliable pattern matching
2. **LLM-based** (optional): More accurate but slower/expensive

#### Chat History Service (`app/services/chat_history.py`)

Manages conversation state:
- Loads session history from Django backend
- Formats conversation for context window
- Saves LLM KV cache (legacy, for Ollama)
- Truncates history to fit context limits

### 4. Client Layer

#### vLLM Client (`app/clients/vllm_client.py`)

Primary LLM interface using OpenAI-compatible API:
- **Streaming generation**: Yields tokens via SSE as they're produced
- **Stateless API**: No KV cache reuse between turns (unlike Ollama)
- **Configurable options**: Temperature, max_tokens, top_p
- **API key support**: For authenticated vLLM endpoints

Key parameters:
```python
{
    "temperature": 0.4,      # Balanced creativity
    "max_tokens": -1,        # Auto (vLLM decides)
    "top_p": 0.9,            # Nucleus sampling
    "model": "qwen3-30b"     # Model identifier
}
```

**Note**: vLLM uses the OpenAI chat completions format (`/v1/chat/completions`) rather than Ollama's generate endpoint.

#### Ollama Client (`app/clients/ollama_client.py`)

Legacy LLM interface (kept for backward compatibility):
- **Local inference**: Runs models locally via Ollama
- **KV cache reuse**: Supports context passing for multi-turn conversations
- **Compatible interface**: Same `generate_once()` and `stream_generate()` signatures

#### OpenSearch Client (`app/clients/opensearch_client.py`)

Simple HTTP client for OpenSearch:
- Basic authentication support
- SSL verification configuration
- Request timeout handling

### 5. Routers

#### Ask Router (`app/routers/ask.py`)

Main chat endpoint implementing:

**Domain Validation**:
```python
is_agri, reason = is_agriculture_related(user_q)
# Logs for analytics but lets LLM handle the response
```

**User Extraction**:
```python
user_uuid = _extract_user_uuid_from_token(auth_token)
# Extracts UUID from JWT for profile loading
```

**Intent Routing**:
```python
intent = classify_intent(query)
if intent == "LLM_ONLY":
    # Direct LLM without search
else:
    # Full RAG pipeline
```

**Caching**:
- Cache key based on: question, context hash, model, parameters, history
- Redis-backed with configurable TTL
- Returns cached responses as simulated streams

**Concurrency Control**:
- Semaphore-based limiting (default: 3 concurrent generations)
- Queue status messages when at capacity

**Citation Processing**:
- Extracts citation patterns: `[1]`, `[1, 2]`, `(source 1)`, `source 2`
- Filters sources to only include cited documents
- Falls back to top-5 if no citations found

**Profile Update** (fire-and-forget):
```python
asyncio.create_task(
    UserProfileService.process_conversation_turn(
        user_uuid, session_id, user_q, full_text, auth_token
    )
)
```

#### TTS Router (`app/routers/tts.py`)

Server-side text-to-speech pipeline:
1. Accepts NDJSON text stream
2. Feeds to Piper TTS (PCM output)
3. Transcodes to WebM/Opus via FFmpeg
4. Streams audio chunks to client

Voice registry supports:
- `en-gb-male` (Alan)
- `en-gb-female` (Alba)
- `en-gb-neutral` (Alan)

### 6. Frontend Architecture

#### Authentication Flow (`static/js/auth.js`, `static/js/login.js`)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User   â”‚â”€â”€â”€â”€â–¶â”‚  /api/login â”‚â”€â”€â”€â”€â–¶â”‚ Django Backend â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â–¼
â”‚  Chat   â”‚â—€â”€â”€â”€â”€â”‚  JWT Store  â”‚â—€â”€â”€â”€â”€â”€ Access/Refresh Tokens
â”‚  Page   â”‚     â”‚ (localStorage)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Token management:
- Access token stored in `localStorage`
- Refresh token for automatic renewal
- Automatic logout on token expiration
- Session management via Django backend proxy

#### Chat Interface (`static/js/chat.js`)

Features:
- **Session management**: Create, load, delete chat sessions
- **Streaming display**: Real-time token rendering
- **Citation rendering**: Converts `[1]` to superscript links
- **Voice integration**: Browser speech synthesis with ğŸ”Š button
- **Auto-focus**: `/` key or any printable key focuses input from anywhere
- **Think time display**: Shows server processing duration

Event handling:
```javascript
// SSE event types
es.addEventListener('status', handleStatus);
es.addEventListener('token', handleToken);
es.addEventListener('sources', handleSources);
es.addEventListener('timing', handleTiming);
es.addEventListener('done', handleDone);
```

#### Voice Support (`static/js/voice.js`)

Comprehensive voice support for 24 EU languages:

**TTS Manager**:
- Browser Web Speech API integration
- Automatic language detection from text
- Manual language override
- Play/pause/stop controls
- Best voice selection per language

**STT Manager**:
- Web Speech Recognition API
- Continuous listening mode
- Interim results display
- 24 EU language support
- Touch-friendly mobile interface

## Data Flow

### Request Lifecycle

```
1. Request Received
   â”œâ”€â”€ Validate authentication (JWT in localStorage/query param)
   â”œâ”€â”€ Extract user UUID from token
   â”œâ”€â”€ Check cache (Redis)
   â””â”€â”€ Initialize streaming response

2. Domain Validation
   â”œâ”€â”€ Check for non-agriculture keywords
   â””â”€â”€ Log result for analytics

3. Intent Classification
   â”œâ”€â”€ Call intent router service
   â””â”€â”€ Determine: RAG or LLM-only

4. User Profile Loading (async)
   â”œâ”€â”€ Fetch profile from Django
   â””â”€â”€ Build profile context string

5. Document Retrieval (RAG path)
   â”œâ”€â”€ Build search query
   â”œâ”€â”€ Query OpenSearch
   â””â”€â”€ Receive ranked documents

6. Context Preparation
   â”œâ”€â”€ Split documents into paragraphs
   â”œâ”€â”€ Rank by relevance
   â”œâ”€â”€ Select top-k chunks
   â””â”€â”€ Format with metadata

7. Prompt Construction
   â”œâ”€â”€ Load conversation history
   â”œâ”€â”€ Inject profile context
   â”œâ”€â”€ Inject context blocks
   â”œâ”€â”€ Apply domain restriction rules
   â”œâ”€â”€ Apply language rules
   â””â”€â”€ Build final prompt

8. Generation
   â”œâ”€â”€ Stream to vLLM
   â”œâ”€â”€ Yield tokens via SSE
   â””â”€â”€ Capture statistics

9. Post-Processing
   â”œâ”€â”€ Extract citations
   â”œâ”€â”€ Filter source list
   â”œâ”€â”€ Update session title (first turn)
   â”œâ”€â”€ Update user profile (fire-and-forget)
   â””â”€â”€ Cache response

10. Response Complete
    â”œâ”€â”€ Emit sources (cited only)
    â”œâ”€â”€ Emit timing metrics
    â””â”€â”€ Close SSE connection
```

## Configuration Architecture

Settings are managed via Pydantic Settings with environment variable support:

```python
class Settings(BaseSettings):
    # Environment
    FA_ENV: str  # local/dev/prd
    
    # OpenSearch
    OPENSEARCH_API_URL: str
    OPENSEARCH_API_USR: str | None
    OPENSEARCH_API_PWD: str | None
    
    # vLLM (Primary)
    VLLM_URL: str
    VLLM_MODEL: str
    VLLM_API_KEY: str | None
    
    # Legacy Ollama
    OLLAMA_URL: str
    LLM_MODEL: str
    
    # Feature flags
    ENABLE_DOCS: bool
    ENABLE_REDOC: bool
```

Environment-specific backends:
| Environment | Django Backend | Auth Endpoint |
|-------------|----------------|---------------|
| local | http://127.0.0.1:8000 | /fastapi/login/ |
| dev | https://backend-admin.dev.farmbook.ugent.be | /fastapi/login/ |
| prd | https://backend-admin.prd.farmbook.ugent.be | /fastapi/login/ |

## Security Considerations

1. **Authentication**: JWT tokens with automatic refresh via Django backend
2. **CORS**: Configured for all origins (adjust for production)
3. **SSL**: Configurable verification for external APIs
4. **Input Sanitization**: HTML escaping in frontend
5. **Rate Limiting**: Concurrency semaphore on generation
6. **Token Storage**: Tokens stored in localStorage (consider httpOnly cookies for enhanced security)

## Performance Optimizations

1. **Response Caching**: Redis-based with SHA-256 keys
2. **Streaming**: Immediate token delivery via SSE
3. **Connection Pooling**: HTTPX async clients with keep-alive
4. **Concurrent Search**: Parallel document fetching
5. **Lazy Loading**: Models loaded on first request
6. **Context Truncation**: History limited to fit context window
7. **Profile Caching**: User profiles cached per request

## Deployment

### Docker Architecture

```dockerfile
# Multi-stage build
FROM python:3.11-slim

# System deps: tini + build toolchain + OpenBLAS runtime + FFmpeg
# Python dependencies from requirements.txt
# Piper voice models downloaded at build time

ENTRYPOINT ["/usr/bin/tini", "--"]
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", 
     "--proxy-headers", "--forwarded-allow-ips", "*", "--timeout-keep-alive", "120"]
```

### External Dependencies

| Service | Purpose | Required |
|---------|---------|----------|
| vLLM | LLM inference | Yes (primary) |
| OpenSearch | Document search | Yes |
| Django Backend | Auth, sessions, profiles | Yes |
| Redis | Response caching | No |
| Intent Router | Query classification | No (falls back to RAG) |
| Ollama | LLM inference (legacy) | No |

## Extension Points

1. **New LLM Backends**: Implement `generate_once()` and `stream_generate()` interface in `app/clients/`
2. **Custom Search**: Extend `SearchService` with additional backends
3. **Additional TTS Voices**: Add entries to `VOICES` registry in `app/routers/tts.py`
4. **Custom Prompts**: Modify templates in `prompt_service.py`
5. **New Routers**: Add API endpoints in `app/routers/`
6. **Enhanced Profile Extraction**: Implement LLM-based extraction in `user_profile_service.py`

## Monitoring & Debugging

- **Health Check**: `GET /health`
- **API Docs**: `/docs` (when enabled)
- **Logs**: Structured logging with configurable levels
- **Timing Metrics**: Included in SSE `timing` events
- **Cache Stats**: Redis key inspection
- **Profile Logging**: User profile updates logged

## Multi-language Architecture

The system supports all 24 EU languages through:

1. **Language Detection**: LLM instructed to respond in user's question language
2. **Translation**: Sources may be in different language; LLM translates content
3. **Voice Support**: 
   - STT: Web Speech Recognition with 24 EU language codes
   - TTS: Browser voices with automatic language detection
4. **Prompt Rules**: Explicit instructions in prompts to maintain language consistency

## Personalization Architecture

User profiles enhance responses without sending full chat history:

```
Conversation â†’ Extraction â†’ Profile Store â†’ Context Building â†’ Prompt Injection
     â”‚              â”‚              â”‚                â”‚                  â”‚
     â–¼              â–¼              â–¼                â–¼                  â–¼
User asks    Keywords/LLM    Django API      Format as string    "User Profile:
question     extract facts    (PostgreSQL)    for prompt          - Expertise: expert
                                                              - Farm: organic dairy"
```

Profile attributes influence:
- Explanation depth (beginner vs expert)
- Technical terminology usage
- Organic vs conventional recommendations
- Regional considerations
